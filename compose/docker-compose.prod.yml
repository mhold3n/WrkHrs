services:
  llm-runner:
    image: vllm/vllm-openai:latest
    ports: [ "8001:8000" ]  # OpenAI-compatible
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=all
      - VLLM_MODEL=${VLLM_MODEL}
      - VLLM_GPU_MEMORY_UTILIZATION=${VLLM_GPU_MEMORY_UTILIZATION}
      - VLLM_MAX_MODEL_LEN=${VLLM_MAX_MODEL_LEN}
      - VLLM_LOGGING_LEVEL=INFO
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - "models:/models"
    restart: unless-stopped
    command: ["--model", "${VLLM_MODEL}", "--trust-remote-code", "--host", "0.0.0.0", "--port", "8000", "--tensor-parallel-size", "2", "--gpu-memory-utilization", "${VLLM_GPU_MEMORY_UTILIZATION}", "--max-model-len", "${VLLM_MAX_MODEL_LEN}", "--dtype", "auto"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s

  asr-api:
    environment:
      - ASR_MODEL=${ASR_MODEL:-medium}
      - ASR_DEVICE=cuda
      - CUDA_VISIBLE_DEVICES=0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped