services:
  llm-runner:
    image: vllm/vllm-openai:latest
    ports: [ "8001:8000" ]  # OpenAI-compatible
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=all
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - "models:/models"
    restart: unless-stopped
    command: [
      "python",
      "-m", "vllm.entrypoints.openai.api_server",
      "--model", "${VLLM_MODEL}",
      "--host", "0.0.0.0",
      "--port", "8000",
      "--max-model-len", "${VLLM_MAX_MODEL_LEN}",
      "--tensor-parallel-size", "${VLLM_TENSOR_PARALLEL_SIZE}",
      "--gpu-memory-utilization", "${VLLM_GPU_MEMORY_UTILIZATION}",
      "--quantization", "${VLLM_QUANTIZATION}",
      "${VLLM_EXTRA_ARGS}"
    ]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s

  asr-api:
    environment:
      - ASR_MODEL=${ASR_MODEL}
      - ASR_DEVICE=cuda
      - CUDA_VISIBLE_DEVICES=0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped