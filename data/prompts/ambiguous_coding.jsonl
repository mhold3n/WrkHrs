{"prompt": "I'm seeing intermittent stalls when the dashboard loads data after a deploy. Locally I can't reproduce it, and synthetic tests pass, but users report the UI 'breathes' every few seconds when filters change. Logs aren't loud. Could we look at what's happening between the API calls and the client render loop and figure out why it feels sticky under normal usage?", "true_intent": "frontend_perf_render_thrash"}
{"prompt": "This job has grown from a quick script into something that touches auth, storage, and billing. It still runs in the request path because 'it was fine at first,' but it's now spiking p95. Can we revisit whether we're doing too much synchronously, and decide which bits should be moved off the hot path without complicating operational triage?", "true_intent": "move_work_off_hot_path"}
{"prompt": "The integration tests pass, but the behavior in production drifts in subtle ways depending on feature flags and region. I'm worried the contract we think we have with the downstream API isn't actually enforced. Could we make the boundary explicit so it's obvious when the provider changes assumptions?", "true_intent": "enforce_api_contracts"}
{"prompt": "This migration worked in staging; in prod the rollout showed data skew we didn't see before. The new index is correct theoretically, but the write pattern during peak traffic looks different than we modeled. Before we tune blindly, what's the smallest instrumentation we can add to make the contention visible?", "true_intent": "instrument_contention_before_tuning"}
{"prompt": "The module handles error paths defensively, but reviewers keep flagging that 'it's hard to tell what's guaranteed.' I suspect we're mixing recoverable input issues with system faults. How can we restructure the flow so callers get predictable outcomes and ops can separate user mistakes from incidents?", "true_intent": "separate_user_errors_from_system_faults"}
{"prompt": "We added caching to reduce backend load, which helped average latency, but users still notice pauses during bursts. I'm not convinced we're caching at the right boundary or expiration. Can we map the read/write patterns and decide whether we should precompute, coalesce, or shift where the cache sits?", "true_intent": "revisit_cache_boundary_and_policy"}
{"prompt": "The CLI tool's UX is decent for happy paths, but when a step fails mid-run, the recovery is murky and we leave temp state around. I'd like a strategy where the user always knows what's safe to retry, and the tool can resume without guessing. What's the minimal checkpointing we can introduce?", "true_intent": "add_checkpoints_and_idempotent_retries"}
{"prompt": "We're parsing untrusted configuration with a permissive loader because it was easy early on. It's now wired into deployment automation. I don't want to overreact, but I'm uneasy about how much surface area we expose. What's the narrowest constrained parser we can adopt without breaking workflows?", "true_intent": "tighten_untrusted_config_parsing"}
{"prompt": "Our tests catch obvious regressions, yet a category of 'it works, but not reliably' keeps escaping—especially around time and concurrency. I don't think we need more tests; we need better seams. Where can we introduce a small interface that makes time and scheduling controllable without mocking everything?", "true_intent": "introduce_time_scheduler_abstractions"}
{"prompt": "The data pipeline backfills have a habit of degrading normal operations. Everyone knows to 'run them off-hours,' but off-hours aren't consistent globally. Can we define guardrails so backfills don't starve interactive traffic, and make that policy visible so SREs aren't guessing on the fly?", "true_intent": "resource_guardrails_for_backfills"}
{"prompt": "The internal API looks simple, but the implicit coupling to global state makes it fragile to reorderings. The team hesitates to clean up because we lack a safe way to change things incrementally. What small refactor would let us isolate just enough state to make progress without a big bang?", "true_intent": "isolate_global_state_incrementally"}
{"prompt": "We're relying on retries to paper over occasional upstream flakiness, but the failures cluster in patterns that suggest we're aligned with their maintenance windows. Rather than keep turning knobs, can we instrument correlation (time, region, method) and agree on a backoff/circuit policy that's observable?", "true_intent": "observable_backoff_and_circuit_breaker"}
{"prompt": "The new search feature returned quickly in benchmarks, yet users report 'lag' when typing. Profiling shows lots of small work on the main thread. I don't think the algorithm is the problem; it's how we stage updates. What's the smallest change that reduces layout thrash without rewriting the component?", "true_intent": "reduce_layout_thrash_and_stage_updates"}
{"prompt": "We adopted a clever metaprogramming trick to reduce boilerplate, and it did—at the cost of new contributors not trusting the code. I'd like to keep the benefit but lower the cognitive load. Can we expose an explicit, boring surface over the dynamic parts so the magic stays contained?", "true_intent": "wrap_metaprogramming_with_explicit_api"}
{"prompt": "Our schema migrations assume linear evolution, but teams ship independently and rebase late. When branches land out of order, the world breaks in non-obvious ways. Could we define a minimal compatibility policy (forward/back) and add checks so a PR can't merge if it violates those invariants?", "true_intent": "schema_compatibility_policy_checks"}
{"prompt": "The analytics job produces numbers that are 'close enough' until finance asks for reconciliation. I suspect we're mixing event-time and processing-time in ways that cancel out at small scale and drift at volume. What's the least disruptive way to align clocks and document the expected skew?", "true_intent": "align_event_and_processing_time"}
{"prompt": "We pushed a JSON-based plugin interface to move faster, but discoverability is poor and error messages are opaque. I don't want a heavy framework, just enough shape so authors know what's valid. What's the lightest schema + lints we can add that improves ergonomics without locking us in?", "true_intent": "add_schema_and_lints_for_plugins"}
{"prompt": "The incident review pointed at gaps in our runbooks more than code defects. We know which signals matter during an outage, but the dashboards hide them behind noise. Before we add more metrics, could we remove or demote the ones that never drive action and make the red-path obvious at 3 a.m.?", "true_intent": "prune_noise_and_surface_runbook_signals"}
